\section{Implementation} \label{sec:implementation}
\subsection{Input}
We model each patch as a DOG quad mesh based on a crease pattern taken from an SVG file containing polylines specifying creases and boundary loops. We use CGAL's arrangement model \cite{cgal} to compute the decomposition induced by the polylines and place an orthogonal grid of a given mesh resolution on top of the entire model, where quads that appear on multiple components are duplicated (see \figref{fig:piecewise_dog_from_crease}). We snap nearby intersections or starting points of non-closed polylines to one another. To get a correct arrangement topology, every curve intersection with other curves or boundary loops needs to be on the orthogonal grid line, i.e. on an edge or a vertex, and we snap their x and y coordinates to prevent the grid from having very thin squares.

\subsection{Optimization problem}
\MiR{What we want to solve (general phrasing, with all constraints taken from before). How we are an in an interesting situation where everything is active and we know nothing should be. We want to stay feasible (motivated by the smooth case) and also not have any linear dependence problem. Adding penalty and also filter out of feasibility. Then approximate the other ones with a smooth functions (and keep the filter). So talk about the heaveside function and the second one.}

\subsection{Equality constrainted SQP}
\MiR{Write down the thing, but make sure to convexify stuff. Use the Jacobian and stuff from that paper, but note that its much better with hessian and lagrangian rather than laplacian, and also convexification is better, and get rid of the lbfgs in porjection. Especially better for higher resolution models.}

\subsection{Objectives and constraints}
\MiR{Gauss-newton hessian for constraint stuff. Convexification for others if needed (maybe write it down in appendix).}

\subsection{Folding constraints}
We employ \theoremref{Thm:supporting_plane} and its discretization eq.\eqref{eq:folding_const} in a simple algorithm to enforce folds along creases while deforming piecewise smooth DOGs. The algorithm tries to minimize an objective, while satisfying the DOG and patches constraints as in \eqref{eq:const_opt} and also ensuring the formation of folds along crease curves. Let $C_{I}$ be all inner crease points, having two neighbours along a crease curve. Let $c_i(x) \in C_{I}$ be a crease point and $\phi_i(x) $ be the folding constraint \eqref{eq:folding_const} for this crease point. We replace \eqref{eq:const_opt} by the following following problem:
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
f(x) \\
& \textrm{subject to} \\
& & g_i(x) = 0, \ \  i = 1, \ldots, m. \\
& & \phi_i(x) = 0, \ \  \forall c_i \in C_{I}
\end{aligned}
\end{equation}

Starting from a feasible flat mesh and motivated by the fact that in the smooth case one cannot move from a folded to a non-folded model once a curve is bend, we strive to always satisfy $\phi_i(x)$ exactly. To that end we do not accept steps in the linesearch that violates $\phi(x)$, and add a penalty term encouraging folds. We replace the non-smooth terms in \eqref{eq:folding_const} by utilizing a smooth approximation for the sign function:
\begin{equation} \label{eq:sign_smooth_approx}
\text{sgn}(x) \approx \tanh(kx)
\end{equation}
For some $k >> 1$, which in practice we fix as $k = 1000$. Hence we replace $\phi_i$ by the approximation:
\begin{equation} \label{eq:folding_const}
\theta_i(x) = \tanh(k\langle e^1,e_f \times e_b \rangle) +  \tanh(k\langle e^2,e_f \times e_b\rangle)
\end{equation}
We then solve the problem:
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
f(x) + \alpha \sum_{c_i \in C_{I}} \theta_i(x)^2\\
& \textrm{s.t.}
\text{ } g_i(x) = 0, \ \  i = 1, \ldots, m.
\end{aligned}
\end{equation}
For some given $\alpha > 0$, where we set our initial $\alpha_0 = 100$. If our linesearch always return a point with unfeasible folding constraints $\phi_(x)$ we increase and set $\alpha = 2\alpha$ \MiR{add an algorithm here?}.
We note that the approximation constraints are also satisfied exactly at the starting point for a flat mesh. Since for a smooth DOG satisfies also the smooth condition \eqref{eq:folding_const_smooth} approximately, the symmetriy of $\tanh$ guarantees us that as long as we model isometries or transformations keeping duplicated edge lengths similar, the constraint $\theta_i(x)$ will approach even faster to zero, even when the curves is slightly bend.

\begin{figure} [h]
	\centering
	\includegraphics[width=\linewidth]{figures/fold_bias_compare}
	\caption{With and without bias. Same deformations. \MiR{todo caption.} }
	\label{fig:fold_bias_compare}
\end{figure}
\subsection{Optimization}
\MiR{
TODO: Explain here about what we do, flows similar to \cite{rabi2018shape}, just use lagrangian where the hessian is first convexified such that (called $H^*$) instead of the DOG Laplacian , which is basically SQP. Use the same strategy to minimally petrubate $J$ to $J^*$ as done in \cite{rabi2018shape}. Quickly go through the details such as merit functions (no need in the "projection" operator with lbfgs), and refer to the appendix for more technical things such as convexification of simple energies. 
At the end we write down our minimization as
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
& & f(x) \\
& \textrm{subject to}
& & g_i(x) = 0, \ \  i = 1, \ldots, m.
\end{aligned}
\end{equation}
}
By subsequently solving KKT systems of the form for a given mesh $x^k$, finding the step direction $d^{k+1}$:
\begin{equation} \label{eq:KKT_eps}
\begin{gathered}
{K} \begin{pmatrix} d^{k+1} \\ \lambda^{k+1} \end{pmatrix}=\mathbf{b} \\
{K}=\begin{pmatrix}
{H^*(x)+\sum\lambda_i^{k} \nabla g_i(x)} & {J^{*\tr}(x)}\\
{J^*(x)} &  0 \\
\end{pmatrix}, \ \ 
\mathbf{b}=\begin{pmatrix}
{\nabla f(x^k)} \\ 
\boldsymbol{-g(x^k)}\\
\end{pmatrix}.
\end{gathered}
\end{equation}
\MiR{
and use a linesearch with a merit function. Say that hard constraints are only dog + edge stitching as in \cite{rabi18} whereas other constraints throughout the paper are enforced by penalizing them, using a convex hessian approximation of gauss-newton (besides the length which are easy to convexify).
}

