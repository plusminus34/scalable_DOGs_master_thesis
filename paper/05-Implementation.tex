\section{Optimization} \label{sec:implementation}
We employ \theoremref{Thm:supporting_plane} and its discretization eq.\eqref{eq:folding_const} in a simple algorithm to enforce folds along creases while deforming piecewise smooth DOGs. The algorithm tries to minimize an objective, while keeping the DOG constraints and ensuring the formation of folds along all crease curves.

\subsection{Problem setup}
We model our curved folded surfaces as a quad mesh, with a separate connected component for each patch. We denote the set of $n$ mesh vertices in $R^3$ by $V \in R^{n\times3}$, the flattened variables by $\x \in R^{3n}$, and the mesh quad faces by $F$. Each connected component by its own is a DOG, i.e. has the connectivity of a subset of $\Z^2$ and satisfy the DOG angle constraints \cite{rabi18}. The patches themselves also need to align along the crease as detailed at \ref{sec:model} enforced by simple linear constraints as done in \cite{rabi2018shape}. For convenience we call both of these constraints the "DOG constraints" and denote them by $\phi_{d_i}(\x) = 0, 1 \leq i \leq m$.

We are interested in deformations that fold along all creases in a given folding pattern using \theoremref{Thm:supporting_plane} and enforcing eq. \eqref{eq:folding_const} on all inner crease points, which are points on crease curves having two neighbours. Let $\phi_{f_i}(\x) = 0, 1 \leq i \leq k$ be the folding constraint \eqref{eq:folding_const} for the inner crease points, together with a few optional M/V assignment constraints \eqref{eq:mountain_valley}.

The problems we solve in this paper can be written in the form:
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x f(x) \\
& \textrm{subject to} \\
& \phi_{d_i}(\x) = 0, \ \  i = 1, \ldots, m. \\
& \phi_{f_i}(\x) = 0, \ \  i = 1, \ldots, k \\ 
\end{aligned}
\end{equation}
With $f$ as an objective, composed of a weighted sum of a bending objective, isometry objective, positional constraints and other terms as specified in \secref{sec:dog_obj}.

\subsection{Folding constraints}
Motivated by the fact that in the smooth case one cannot move from a folded to a non-folded model once a curve is bend, we strive to always satisfy $\phi_{f_i}(x) = 0$ exactly. If a crease is locally already folded then any small deformation keeps it that way, and conversely if a crease is not folded then any small deformation doesn't change that. This is the general case both for a smooth curved folded surface, and for a piecewise DOG using eq. \eqref{eq:folding_const}.\MiR{should this important fact be stated earlier? in the discretization?}. Hence, folding is usually an open condition, satisfied in some neighbourhood. The exception to this rule, both in the smooth case and in our discretization, is the tangents of the surfaces lie on the osculating plane of the curve, which in the smooth case is equal to the point having zero normal curvature. An important case where this occurs is a flat surface, which is our typical starting point.

 We note that we could have formulated our folding constraints using \emph{non-strict} inequalities. For instance the constraint \eqref{eq:folding_const} could be replaced by:

 Using an inequality formulation, a flat piecewise DOG is a bifurcation point in any optimization. At a flat configuration, the set of constraints are all "active", i.e. the osculating plane of the creases exactly
Bending a single patch on a piecewise smooth developable surface

Starting from a feasible flat mesh and 
\MiR{How we are an in an interesting situation where everything is active and we know nothing should be. Bifurcation point. We want to stay feasible (motivated by the smooth case) and also not have any linear dependence problem. Adding penalty and also filter out of feasibility. Then approximate the other ones with a smooth functions (and keep the filter). So talk about the heaveside function and the second one.}

 We replace \eqref{eq:const_opt} by the following following problem:
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
f(x) \\
& \textrm{subject to} \\
& & g_i(x) = 0, \ \  i = 1, \ldots, m. \\
& & \phi_i(x) = 0, \ \  \forall c_i \in C_{I}
\end{aligned}
\end{equation}

 To that end we do not accept steps in the linesearch that violates $\phi(x)$, and add a penalty term encouraging folds. We replace the non-smooth terms in \eqref{eq:folding_const} by utilizing a smooth approximation for the sign function:
\begin{equation} \label{eq:sign_smooth_approx}
\text{sgn}(x) \approx \tanh(kx)
\end{equation}
For some $k >> 1$, which in practice we fix as $k = 1000$. Hence we replace $\phi_i$ by the approximation:
\begin{equation} \label{eq:folding_const_smooth}
\theta_i(x) = \tanh(k\langle e^1,e_f \times e_b \rangle) +  \tanh(k\langle e^2,e_f \times e_b\rangle)
\end{equation}
We then solve the problem:
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
f(x) + \alpha \sum_{c_i \in C_{I}} \theta_i(x)^2\\
& \textrm{s.t.}
\text{ } g_i(x) = 0, \ \  i = 1, \ldots, m.
\end{aligned}
\end{equation}
For some given $\alpha > 0$, where we set our initial $\alpha_0 = 100$. If our linesearch always return a point with unfeasible folding constraints $\phi_(x)$ we increase and set $\alpha = 2\alpha$ \MiR{add an algorithm here?}.
We note that the approximation constraints are also satisfied exactly at the starting point for a flat mesh. Since for a smooth DOG satisfies also the smooth condition \eqref{eq:folding_const_smooth} approximately, the symmetriy of $\tanh$ guarantees us that as long as we model isometries or transformations keeping duplicated edge lengths similar, the constraint $\theta_i(x)$ will approach even faster to zero, even when the curves is slightly bend.

\subsection{Equality constrainted SQP}
\MiR{Write down the thing, but make sure to convexify stuff. Use the Jacobian and stuff from that paper, but note that its much better with hessian and lagrangian rather than laplacian, and also convexification is better, and get rid of the lbfgs in porjection. Especially better for higher resolution models.}

\subsection{Objectives and constraints} \label{sec:dog_obj}
\MiR{Gauss-newton hessian for constraint stuff. Convexification for others if needed (maybe write it down in appendix).}

\begin{figure} [h]
	\centering
	\includegraphics[width=\linewidth]{figures/fold_bias_compare}
	\caption{With and without bias. Same deformations. \MiR{todo caption.} }
	\label{fig:fold_bias_compare}
\end{figure}
\subsection{Optimization}
\MiR{
TODO: Explain here about what we do, flows similar to \cite{rabi2018shape}, just use lagrangian where the hessian is first convexified such that (called $H^*$) instead of the DOG Laplacian , which is basically SQP. Use the same strategy to minimally petrubate $J$ to $J^*$ as done in \cite{rabi2018shape}. Quickly go through the details such as merit functions (no need in the "projection" operator with lbfgs), and refer to the appendix for more technical things such as convexification of simple energies. 
At the end we write down our minimization as
\begin{equation} \label{eq:const_opt}
\begin{aligned}
& \argmin_x
& & f(x) \\
& \textrm{subject to}
& & g_i(x) = 0, \ \  i = 1, \ldots, m.
\end{aligned}
\end{equation}
}
By subsequently solving KKT systems of the form for a given mesh $x^k$, finding the step direction $d^{k+1}$:
\begin{equation} \label{eq:KKT_eps}
\begin{gathered}
{K} \begin{pmatrix} d^{k+1} \\ \lambda^{k+1} \end{pmatrix}=\mathbf{b} \\
{K}=\begin{pmatrix}
{H^*(x)+\sum\lambda_i^{k} \nabla g_i(x)} & {J^{*\tr}(x)}\\
{J^*(x)} &  0 \\
\end{pmatrix}, \ \ 
\mathbf{b}=\begin{pmatrix}
{\nabla f(x^k)} \\ 
\boldsymbol{-g(x^k)}\\
\end{pmatrix}.
\end{gathered}
\end{equation}
\MiR{
and use a linesearch with a merit function. Say that hard constraints are only dog + edge stitching as in \cite{rabi18} whereas other constraints throughout the paper are enforced by penalizing them, using a convex hessian approximation of gauss-newton (besides the length which are easy to convexify).
}